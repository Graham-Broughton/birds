{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25700/1897915493.py:20: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# os.environ['TPU_LOG_DIR'] = '/kaggle/working'\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # avoids assignment warning\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # enable progress bars in pandas operations\n",
    "import gc\n",
    "\n",
    "import librosa\n",
    "import sklearn\n",
    "import json\n",
    "\n",
    "# Import for visualization\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.cm.get_cmap('coolwarm')\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as lid\n",
    "import IPython.display as ipd\n",
    "import cv2\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "# Set logging level to avoid unnecessary messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "# Set autograph verbosity to avoid unnecessary messages\n",
    "tf.autograph.set_verbosity(0)\n",
    "# Enable xla for speed up\n",
    "# tf.config.optimizer.set_jit(False) #  throws error for time-freq-mask\n",
    "\n",
    "# Import required tensorflow modules\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Import KaggleDatasets for accessing Kaggle datasets\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# WandB for experiment tracking\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('/mnt/disk')\n",
    "bc_21 = DATA / \"birdclef-2021\"\n",
    "bc_22 = DATA / \"birdclef-2022\"\n",
    "bc_23 = DATA / \"birdclef-2023\"\n",
    "bsr = DATA / \"birdsong-recognition\"\n",
    "canto = DATA / \"canto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np: 1.23.5\n",
      "pd: 2.0.1\n",
      "sklearn: 1.2.2\n",
      "librosa: 0.10.0.post2\n",
      "tf: 2.12.0\n",
      "tfp: 0.20.1\n",
      "tfa: 0.20.0\n",
      "tfio: 0.32.0\n",
      "w&b: 0.15.3\n"
     ]
    }
   ],
   "source": [
    "print('np:', np.__version__)\n",
    "print('pd:', pd.__version__)\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('librosa:', librosa.__version__)\n",
    "print('tf:', tf.__version__)\n",
    "print('tfp:', tfp.__version__)\n",
    "print('tfa:', tfa.__version__)\n",
    "print('tfio:', tfio.__version__)\n",
    "print('w&b:', wandb.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Debugging\n",
    "    debug = False\n",
    "    \n",
    "    # Verbosity level\n",
    "    verbose = 0\n",
    "    \n",
    "    # Plot training history\n",
    "    training_plot = True\n",
    "    \n",
    "    # Weights and Biases logging\n",
    "    wandb = True\n",
    "    competition   = 'birdclef-2023' \n",
    "    _wandb_kernel = 'awsaf49'\n",
    "    \n",
    "    # Experiment name and comment\n",
    "    exp_name = 'birdclef-pretrain-v3'\n",
    "    comment = 'EfficientNetB1|No-FSR|t=10s|128x384|cutmix'\n",
    "    # Notebook link\n",
    "    \n",
    "    # Device and random seed\n",
    "    device = 'TPU-VM'\n",
    "    seed = 42\n",
    "    \n",
    "    # Input image size and batch size\n",
    "    img_size = [128, 384]\n",
    "    batch_size = 32\n",
    "    upsample_thr = 50 # min sample of each class (upsample)\n",
    "    cv_filter = True # always keeps low sample data in train\n",
    "    \n",
    "    # Audio duration, sample rate, and length\n",
    "    duration = 10 # second\n",
    "    sample_rate = 32000\n",
    "    audio_len = duration*sample_rate\n",
    "    \n",
    "    # STFT parameters\n",
    "    nfft = 2028\n",
    "    window = 2048\n",
    "    hop_length = audio_len // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    normalize = True\n",
    "    \n",
    "    # Inference batch size, test time augmentation, and drop remainder\n",
    "    infer_bs = 2\n",
    "    tta = 1\n",
    "    drop_remainder = True\n",
    "    \n",
    "    # Number of epochs, model name, and number of folds\n",
    "    epochs = 25\n",
    "    model_name = 'EfficientNetB1'\n",
    "    fsr = False # reduce stride of stem block\n",
    "    num_fold = 5\n",
    "    \n",
    "    # Selected folds for training and evaluation\n",
    "    selected_folds = [0]\n",
    "\n",
    "    # Pretraining, neck features, and final activation function\n",
    "    pretrain = 'imagenet'\n",
    "    neck_features = 0\n",
    "    final_act = 'softmax'\n",
    "    \n",
    "    # Learning rate, optimizer, and scheduler\n",
    "    lr = 1e-3\n",
    "    scheduler = 'cos'\n",
    "    optimizer = 'Adam' # AdamW, Adam\n",
    "    \n",
    "    # Loss function and label smoothing\n",
    "    loss = 'CCE' # BCE, CCE\n",
    "    label_smoothing = 0.05 # label smoothing\n",
    "    \n",
    "    # Data augmentation parameters\n",
    "    augment=True\n",
    "    \n",
    "    # Time Freq masking\n",
    "    freq_mask_prob=0.50\n",
    "    num_freq_masks=1\n",
    "    freq_mask_param=10\n",
    "    time_mask_prob=0.50\n",
    "    num_time_masks=2\n",
    "    time_mask_param=25\n",
    "\n",
    "    # Audio Augmentation Settings\n",
    "    audio_augment_prob = 0.5\n",
    "    \n",
    "    mixup_prob = 0.65\n",
    "    mixup_alpha = 0.5\n",
    "    \n",
    "    cutmix_prob = 0.65\n",
    "    cutmix_alpha = 2.5\n",
    "    \n",
    "    timeshift_prob = 0.0\n",
    "    \n",
    "    gn_prob = 0.35\n",
    "\n",
    "    # Class Labels for BirdCLEF 23\n",
    "    class_names = sorted(glob(str(bc_23 / 'train_audio') + '/*'))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "    \n",
    "    # Class Labels for BirdCLEF 21 & 22\n",
    "    class_names2 = sorted(set(glob(str(bc_21 / 'train_short_audio') + '/*')\n",
    "                       + glob(str(bc_22 / 'train_audio') + '/*')\n",
    "                       + glob(str(bsr / 'train_audio') + '/*')))\n",
    "    num_classes2 = len(class_names2)\n",
    "    class_labels2 = list(range(num_classes2))\n",
    "    label2name2 = dict(zip(class_labels2, class_names2))\n",
    "    name2label2 = {v:k for k,v in label2name2.items()}\n",
    "\n",
    "    # Training Settings\n",
    "    target_col = ['target']\n",
    "    tab_cols = ['filename']\n",
    "    monitor = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wandb library for logging and tracking experiments\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"Detect and intializes GPU/TPU automatically\"\n",
    "    # Check TPU category\n",
    "    tpu = 'local' if CFG.device=='TPU-VM' else None\n",
    "    try:\n",
    "        # Connect to TPU\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu) \n",
    "        # Set TPU strategy\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print(f'> Running on {CFG.device}', tpu.master(), end=' | ')\n",
    "        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n",
    "        device=CFG.device\n",
    "    except:\n",
    "        # If TPU is not available, detect GPUs\n",
    "        gpus = tf.config.list_logical_devices('GPU')\n",
    "        ngpu = len(gpus)\n",
    "         # Check number of GPUs\n",
    "        if ngpu:\n",
    "            # Set GPU strategy\n",
    "            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
    "            # Print GPU details\n",
    "            print(\"> Running on GPU\", end=' | ')\n",
    "            print(\"Num of GPUs: \", ngpu)\n",
    "            device='GPU'\n",
    "        else:\n",
    "            # If no GPUs are available, use CPU\n",
    "            print(\"> Running on CPU\")\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            device='CPU'\n",
    "    return strategy, device, tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on CPU\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU/TPU/TPU-VM\n",
    "strategy, CFG.device, tpu = get_device()\n",
    "CFG.replicas = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path('/mnt/disk/birds/data')\n",
    "BASE_PATH0 = BASE / 'birdsong-recognition'\n",
    "BASE_PATH1 = BASE / 'birdclef-2021'\n",
    "BASE_PATH2 = BASE / 'birdclef-2022'\n",
    "BASE_PATH3 = BASE / 'birdclef-2023'\n",
    "BASE_PATH4 = BASE / 'canto'\n",
    "\n",
    "if CFG.device==\"TPU\":\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "    GCS_PATH0 = KaggleDatasets().get_gcs_path(BASE_PATH0.split('/')[-1])\n",
    "    GCS_PATH1 = KaggleDatasets().get_gcs_path(BASE_PATH1.split('/')[-1])\n",
    "    GCS_PATH2 = KaggleDatasets().get_gcs_path(BASE_PATH2.split('/')[-1])\n",
    "    GCS_PATH3 = KaggleDatasets().get_gcs_path(BASE_PATH3.split('/')[-1])\n",
    "    GCS_PATH4 = KaggleDatasets().get_gcs_path(BASE_PATH4.split('/')[-1])\n",
    "    GCS_PATH5 = KaggleDatasets().get_gcs_path(BASE_PATH5.split('/')[-1])\n",
    "else:\n",
    "    GCS_PATH0 = BASE_PATH0\n",
    "    GCS_PATH1 = BASE_PATH1\n",
    "    GCS_PATH2 = BASE_PATH2\n",
    "    GCS_PATH3 = BASE_PATH3\n",
    "    GCS_PATH4 = BASE_PATH4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m df_23[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_23\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m df_23[\u001b[39m'\u001b[39m\u001b[39mxc_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_23\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m \u001b[39massert\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(df_23\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[39m# Display rwos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m# Samples in BirdCLEF 23: \u001b[39m\u001b[39m{:,}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(df_23)))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_23 = pd.read_csv(BASE_PATH3 / 'train_metadata.csv')\n",
    "df_23['filepath'] = str(GCS_PATH3 / 'train_audio' / df_23.filename)\n",
    "df_23['target'] = df_23.primary_label.map(CFG.name2label)\n",
    "df_23['birdclef'] = '23'\n",
    "df_23['filename'] = df_23.filepath.map(lambda x: x.split('/')[-1])\n",
    "df_23['xc_id'] = df_23.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "assert tf.io.gfile.exists(df_23.filepath.iloc[0])\n",
    "\n",
    "# Display rwos\n",
    "print(\"# Samples in BirdCLEF 23: {:,}\".format(len(df_23)))\n",
    "df_23.head(2).style.set_caption(\"BirdCLEF - 23\").set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '16px')\n",
    "    ]\n",
    "}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BirdCLEF-2020\n",
    "df_20 = pd.read_csv(BASE_PATH0 / 'train.csv')\n",
    "df_20['primary_label'] = df_20['ebird_code']\n",
    "df_20['filepath'] = str(GCS_PATH0 / 'train_audio' / df_20.primary_label / df_20.filename)\n",
    "df_20['scientific_name'] = df_20['sci_name']\n",
    "df_20['common_name'] = df_20['species']\n",
    "df_20['target'] = df_20.primary_label.map(CFG.name2label2)\n",
    "df_20['birdclef'] = '20'\n",
    "assert tf.io.gfile.exists(df_20.filepath.iloc[0])\n",
    "\n",
    "# Xeno-Canto Extend by @vopani\n",
    "df_xc = pd.read_csv(BASE_PATH4 / 'train_extended.csv')\n",
    "df_xc['filepath'] = str(GCS_PATH4 / df_xc.ebird_code / df_xc.filename)\n",
    "df_xc['primary_label'] = df_xc['ebird_code']\n",
    "df_xc['scientific_name'] = df_xc['sci_name']\n",
    "df_xc['common_name'] = df_xc['species']\n",
    "df_xc['target'] = df_xc.primary_label.map(CFG.name2label2)\n",
    "df_xc['birdclef'] = 'xc'\n",
    "assert tf.io.gfile.exists(df_xc.filepath.iloc[0])\n",
    "\n",
    "# BirdCLEF-2021\n",
    "df_21 = pd.read_csv(BASE_PATH1 / 'train_metadata.csv')\n",
    "df_21['filepath'] = str(GCS_PATH1 / 'train_short_audio' / df_21.primary_label / df_21.filename)\n",
    "df_21['target'] = df_21.primary_label.map(CFG.name2label2)\n",
    "df_21['birdclef'] = '21'\n",
    "corrupt_paths = ['/kaggle/input/birdclef-2021/train_short_audio/houwre/XC590621.ogg',\n",
    "                 '/kaggle/input/birdclef-2021/train_short_audio/cogdov/XC579430.ogg']\n",
    "df_21 = df_21[~df_21.filepath.isin(corrupt_paths)] # remove all zero audios\n",
    "assert tf.io.gfile.exists(df_21.filepath.iloc[0])\n",
    "\n",
    "# BirdCLEF-2022\n",
    "df_22 = pd.read_csv(BASE_PATH2 / 'train_metadata.csv')\n",
    "df_22['filepath'] = str(GCS_PATH2 / 'train_audio' / df_22.filename)\n",
    "df_22['target'] = df_22.primary_label.map(CFG.name2label2)\n",
    "df_22['birdclef'] = '22'\n",
    "assert tf.io.gfile.exists(df_22.filepath.iloc[0])\n",
    "\n",
    "# Merge 2021 and 2022 for pretraining\n",
    "df_pre = pd.concat([df_20, df_21, df_22, df_xc], axis=0, ignore_index=True)\n",
    "df_pre['filename'] = df_pre.filepath.map(lambda x: x.split('/')[-1])\n",
    "df_pre['xc_id'] = df_pre.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "nodup_idx = df_pre[['xc_id','primary_label','author']].drop_duplicates().index\n",
    "df_pre = df_pre.loc[nodup_idx].reset_index(drop=True)\n",
    "\n",
    "# # Remove duplicates\n",
    "df_pre = df_pre[~df_pre.xc_id.isin(df_23.xc_id)].reset_index(drop=True)\n",
    "corrupt_mp3s = json.load(open('/b-calls/birdclef-corrupt-mp3-files-ds/corrupt_mp3_files.json','r'))\n",
    "df_pre = df_pre[~df_pre.filepath.isin(corrupt_mp3s)]\n",
    "df_pre = df_pre[['filename','filepath','primary_label','secondary_labels',\n",
    "                 'rating','author','file_type','xc_id','scientific_name',\n",
    "                'common_name','target','birdclef','bird_seen']]\n",
    "# Display rows\n",
    "print(\"# Samples for Pre-Training: {:,}\".format(len(df_pre)))\n",
    "df_pre.head(2).style.set_caption(\"Pre-Training Data\").set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '16px')\n",
    "    ]\n",
    "}])\n",
    "\n",
    "# Show distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "df_pre.birdclef.value_counts().plot.bar(color=[cmap(0.0),cmap(0.25), cmap(0.65), cmap(0.9)])\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Dataset distribution for Pre-Training\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize the StratifiedKFold object with 5 splits and shuffle the data\n",
    "skf1 = StratifiedKFold(n_splits=25, shuffle=True, random_state=CFG.seed)\n",
    "skf2 = StratifiedKFold(n_splits=CFG.num_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_pre = df_pre.reset_index(drop=True)\n",
    "df_23 = df_23.reset_index(drop=True)\n",
    "\n",
    "# Create a new column in the dataframe to store the fold number for each row\n",
    "df_pre[\"fold\"] = -1\n",
    "df_23[\"fold\"] = -1\n",
    "\n",
    "# BirdCLEF - 21 & 22\n",
    "for fold, (train_idx, val_idx) in enumerate(skf1.split(df_pre, df_pre['primary_label'])):\n",
    "    df_pre.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "# IBirdCLEF - 23\n",
    "for fold, (train_idx, val_idx) in enumerate(skf2.split(df_23, df_23['primary_label'])):\n",
    "    df_23.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, thr=5):\n",
    "    # Count the number of samples for each class\n",
    "    counts = df.primary_label.value_counts()\n",
    "\n",
    "    # Condition that selects classes with less than `thr` samples\n",
    "    cond = df.primary_label.isin(counts[counts<thr].index.tolist())\n",
    "\n",
    "    # Add a new column to select samples for cross validation\n",
    "    df['cv'] = True\n",
    "\n",
    "    # Set cv = False for those class where there is samples less than thr\n",
    "    df.loc[cond, 'cv'] = False\n",
    "\n",
    "    # Return the filtered dataframe\n",
    "    return df\n",
    "    \n",
    "def upsample_data(df, thr=20):\n",
    "    # get the class distribution\n",
    "    class_dist = df['primary_label'].value_counts()\n",
    "\n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    down_classes = class_dist[class_dist < thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    up_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in down_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"primary_label==@c\")\n",
    "        # find number of samples to add\n",
    "        num_up = thr - class_df.shape[0]\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=num_up, replace=True, random_state=CFG.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        up_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    up_df = pd.concat([df] + up_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return up_df\n",
    "\n",
    "def downsample_data(df, thr=500):\n",
    "    # get the class distribution\n",
    "    class_dist = df['primary_label'].value_counts()\n",
    "    \n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    up_classes = class_dist[class_dist > thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    down_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in up_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"primary_label==@c\")\n",
    "        # Remove that class data\n",
    "        df = df.query(\"primary_label!=@c\")\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=thr, replace=False, random_state=CFG.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        down_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    down_df = pd.concat([df] + down_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return down_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates random integer\n",
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "\n",
    "# Generats random float\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow_extra as tfe\n",
    "\n",
    "# Randomly shift audio -> any sound at <t> time may get shifted to <t+shift> time\n",
    "@tf.function\n",
    "def TimeShift(audio, prob=0.5):\n",
    "    # Randomly apply time shift with probability `prob`\n",
    "    if random_float() < prob:\n",
    "        # Calculate random shift value\n",
    "        shift = random_int(shape=[], minval=0, maxval=tf.shape(audio)[0])\n",
    "        # Randomly set the shift to be negative with 50% probability\n",
    "        if random_float() < 0.5:\n",
    "            shift = -shift\n",
    "        # Roll the audio signal by the shift value\n",
    "        audio = tf.roll(audio, shift, axis=0)\n",
    "    return audio\n",
    "\n",
    "# Apply random noise to audio data\n",
    "@tf.function\n",
    "def GaussianNoise(audio, std=[0.0025, 0.025], prob=0.5):\n",
    "    # Select a random value of standard deviation for Gaussian noise within the given range\n",
    "    std = random_float([], std[0], std[1])\n",
    "    # Randomly apply Gaussian noise with probability `prob`\n",
    "    if random_float() < prob:\n",
    "        # Add random Gaussian noise to the audio signal\n",
    "        GN = tf.keras.layers.GaussianNoise(stddev=std)\n",
    "        audio = GN(audio, training=True) # training=False don't apply noise to data\n",
    "    return audio\n",
    "\n",
    "# Applies augmentation to Audio Signal\n",
    "def AudioAug(audio):\n",
    "    # Apply time shift and Gaussian noise to the audio signal\n",
    "    audio = TimeShift(audio, prob=CFG.timeshift_prob)\n",
    "    audio = GaussianNoise(audio, prob=CFG.gn_prob)\n",
    "    return audio\n",
    "\n",
    "# CutMix & MixUp\n",
    "mixup_layer = tfe.layers.MixUp(alpha=CFG.mixup_alpha, prob=CFG.mixup_prob)\n",
    "cutmix_layer = tfe.layers.CutMix(alpha=CFG.cutmix_alpha, prob=CFG.cutmix_prob)\n",
    "\n",
    "def CutMixUp(audios, labels):\n",
    "    audios, labels = mixup_layer(audios, labels, training=True)\n",
    "    audios, labels = cutmix_layer(audios, labels, training=True)\n",
    "    return audios, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes Audio\n",
    "def audio_decoder(with_labels=True, dim=CFG.audio_len, \n",
    "                  take_first=False, num_classes=264, CFG=CFG):\n",
    "    def get_audio(filepath):\n",
    "        ftype = filepath[1]\n",
    "        filepath = filepath[0]\n",
    "        file_bytes = tf.io.read_file(filepath)\n",
    "        if ftype:\n",
    "            audio = tfio.audio.decode_vorbis(file_bytes) # decode .ogg file\n",
    "        else:\n",
    "            audio = tfio.audio.decode_mp3(file_bytes) # decode .mp3 file\n",
    "        audio = tf.cast(audio, tf.float32)\n",
    "        if tf.shape(audio)[1]>1: # stereo -> mono\n",
    "            audio = audio[...,0:1]\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        return audio\n",
    "    \n",
    "    def crop_or_pad(audio, target_len, pad_mode='constant', take_first=True):\n",
    "        audio_len = tf.shape(audio)[0]\n",
    "        diff_len = abs(target_len - audio_len)\n",
    "        if audio_len < target_len:\n",
    "            pad1 = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "            pad2 = diff_len - pad1\n",
    "            audio = tf.pad(audio, paddings=[[pad1, pad2]], mode=pad_mode)\n",
    "        elif audio_len > target_len:\n",
    "            if take_first:\n",
    "                audio = audio[:target_len]\n",
    "            else:\n",
    "                idx = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "                audio = audio[idx: (idx + target_len)]\n",
    "        return tf.reshape(audio, [target_len])\n",
    "\n",
    "    def get_target(target):          \n",
    "        target = tf.reshape(target, [1])\n",
    "        target = tf.cast(tf.one_hot(target, num_classes), tf.float32) \n",
    "        target = tf.reshape(target, [num_classes])\n",
    "        return target\n",
    "\n",
    "    def decode(path):\n",
    "        audio = get_audio(path)\n",
    "        audio = crop_or_pad(audio, dim) # crop or pad audio to keep a fixed length\n",
    "        audio = tf.reshape(audio, [dim])\n",
    "        return audio\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        label = get_target(label)\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies augmentation to audio\n",
    "def audio_augmenter(with_labels=True, dim=CFG.audio_len, CFG=CFG):\n",
    "    def augment(audio, dim=dim):\n",
    "        if random_float() <= CFG.audio_augment_prob:\n",
    "            audio = AudioAug(audio)\n",
    "        audio = tf.reshape(audio, [dim])\n",
    "        return audio\n",
    "    \n",
    "    def augment_with_labels(audio, label):    \n",
    "        return augment(audio), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(paths, ftype, labels=None, batch_size=32, target_size=[128, 256], \n",
    "                  audio_decode_fn=None, audio_augment_fn=None,\n",
    "                  take_first=False, num_classes=264,\n",
    "                  cache=True, cache_dir=\"\",drop_remainder=False,\n",
    "                  augment=True, repeat=True, shuffle=1024):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from the given paths and labels.\n",
    "    \n",
    "    Args:\n",
    "        paths (list): A list of file paths to the audio files.\n",
    "        labels (list): A list of corresponding labels for the audio files.\n",
    "        batch_size (int): Batch size for the created dataset.\n",
    "        target_size (list): A list of target image size for the spectrograms.\n",
    "        audio_decode_fn (function): A function to decode the audio file.\n",
    "        audio_augment_fn (function): A function to augment the audio file.\n",
    "        cache (bool): Whether to cache the dataset or not.\n",
    "        cache_dir (str): Directory path to cache the dataset.\n",
    "        drop_remainder (bool): Whether to drop the last batch if it is smaller than batch_size.\n",
    "        augment (bool): Whether to augment the dataset or not.\n",
    "        repeat (bool): Whether to repeat the dataset or not.\n",
    "        shuffle (int): Number of elements from the dataset to buffer for shuffling.\n",
    "        \n",
    "    Returns:\n",
    "        ds (tf.data.Dataset): A TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    # Create cache directory if cache is enabled\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    # Set default audio decode function if not provided\n",
    "    if audio_decode_fn is None:\n",
    "        audio_decode_fn = audio_decoder(labels is not None,\n",
    "                                        dim=CFG.audio_len, \n",
    "                                        take_first=take_first,\n",
    "                                        num_classes=num_classes,\n",
    "                                        CFG=CFG)\n",
    "    # Set default audio augmentation function if not provided\n",
    "    if audio_augment_fn is None:\n",
    "        audio_augment_fn = audio_augmenter(labels is not None, \n",
    "                                           dim=CFG.audio_len, CFG=CFG)\n",
    "        \n",
    "    # Set TensorFlow AUTOTUNE option\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    # Create slices based on whether labels are provided\n",
    "    slices = ((paths, ftype),) if labels is None else ((paths, ftype), labels)\n",
    "    # Create TensorFlow dataset from slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    # Map audio decode function to dataset\n",
    "    ds = ds.map(audio_decode_fn, num_parallel_calls=AUTO)\n",
    "    # Cache dataset in memory if cache is enabled\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    # Repeat dataset indefinitely if repeat is enabled\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    # Create TensorFlow dataset options\n",
    "    opt = tf.data.Options()\n",
    "    # Shuffle dataset if shuffle is enabled\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt.experimental_deterministic = False\n",
    "    if CFG.device=='GPU':\n",
    "        # If the device is a GPU, turn off auto-sharding to avoid performance issues\n",
    "        opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    # Set the options for the dataset\n",
    "    ds = ds.with_options(opt)\n",
    "    # Apply audio augmentation to the dataset if augment is True\n",
    "    ds = ds.map(audio_augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    # Batch the dataset with the specified batch size\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    # Apply MixUp & CutMix regularization to the dataset\n",
    "    if augment and labels is not None:\n",
    "        ds = ds.map(CutMixUp,num_parallel_calls=AUTO)\n",
    "    # Prefetch the next batch of data to improve performance\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Aides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch, row=3, col=3, label2name=None,):\n",
    "    \"\"\"Plot one batch data\"\"\"\n",
    "    if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "        audios, tars = batch\n",
    "    else:\n",
    "        audios = batch\n",
    "        tars = None\n",
    "    plt.figure(figsize=(col*5, row*3))\n",
    "    for idx in range(row*col):\n",
    "        ax = plt.subplot(row, col, idx+1)\n",
    "        plt.plot(audios[idx].numpy(), color=cmap(0.1))\n",
    "        if tars is not None:\n",
    "            label = tars[idx].numpy().argmax()\n",
    "            name = label2name[label]\n",
    "            plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_history(history):\n",
    "    \"\"\"Plot trainign history, credit: @cdeotte\"\"\"\n",
    "    epochs = len(history.history['auc'])\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(np.arange(epochs),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
    "    plt.plot(np.arange(epochs),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n",
    "    x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
    "    plt.ylabel('AUC (PR)',size=14); plt.xlabel('Epoch',size=14)\n",
    "    plt.legend(loc=2)\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(np.arange(epochs),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "    plt2.plot(np.arange(epochs),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
    "    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "    plt.ylabel('Loss',size=14)\n",
    "    plt.title('Fold %i - Training Plot'%(fold+1),size=18)\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_layer = tfe.layers.MelSpectrogram(n_fft=CFG.nfft, \n",
    "                                          hop_length=CFG.hop_length, \n",
    "                                          sr=CFG.sample_rate, \n",
    "                                          ref=1.0,\n",
    "                                          fmin=500,\n",
    "                                          fmax=15000,\n",
    "                                          out_channels=3)\n",
    "specs = melspec_layer(audios)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(12, 5))\n",
    "lid.waveshow(audios[0].numpy(), sr=CFG.sample_rate, ax=ax[0], axis=None)\n",
    "\n",
    "lid.specshow(specs[0, ..., 0].numpy(), \n",
    "             n_fft=CFG.nfft, \n",
    "             hop_length=CFG.hop_length, \n",
    "             sr=CFG.sample_rate,\n",
    "             x_axis='time',\n",
    "             y_axis='mel',\n",
    "             cmap='coolwarm',\n",
    "              ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Frequency Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_layer = tfe.layers.TimeFreqMask(freq_mask_prob=0.65,\n",
    "                                  num_freq_masks=2,\n",
    "                                  freq_mask_param=10,\n",
    "                                  time_mask_prob=0.65,\n",
    "                                  num_time_masks=3,\n",
    "                                  time_mask_param=25,\n",
    "                                  time_last=True,)\n",
    "specs2 = tfm_layer(specs, training=True)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "lid.specshow(specs2[0, ..., 0].numpy(), \n",
    "             n_fft=CFG.nfft, \n",
    "             hop_length=CFG.hop_length, \n",
    "             sr=CFG.sample_rate,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tfe.layers.ZScoreMinMax()\n",
    "specs3 = norm_layer(specs2)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.hist(specs2.numpy().ravel(), alpha=0.8, color=cmap(0.1))\n",
    "plt.hist(specs3.numpy().ravel(), alpha=0.8, color=cmap(0.9))\n",
    "plt.legend([\"w/o normalize\", \"w/ normalize\"])\n",
    "plt.semilogx()\n",
    "plt.title(\"Effect of Normalization\")\n",
    "plt.xlabel(\"Pixel Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Loss, Optimizer and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def get_metrics():\n",
    "#     acc = tf.keras.metrics.BinaryAccuracy(name='acc')\n",
    "    auc = tf.keras.metrics.AUC(curve='PR', name='auc', multi_label=False) # auc on prcision-recall curve\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy(name='acc')\n",
    "    return [acc, auc]\n",
    "\n",
    "def padded_cmap(y_true, y_pred, padding_factor=5):\n",
    "    num_classes = y_true.shape[1]\n",
    "    pad_rows = np.array([[1]*num_classes]*padding_factor)\n",
    "    y_true = np.concatenate([y_true, pad_rows])\n",
    "    y_pred = np.concatenate([y_pred, pad_rows])\n",
    "    score = sklearn.metrics.average_precision_score(y_true, y_pred, average='macro',)\n",
    "    return score\n",
    "\n",
    "def get_loss():\n",
    "    if CFG.loss==\"CCE\":\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=CFG.label_smoothing)\n",
    "    elif CFG.loss==\"BCE\":\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=CFG.label_smoothing)\n",
    "    else:\n",
    "        raise ValueError(\"Loss not found\")\n",
    "    return loss\n",
    "    \n",
    "def get_optimizer():\n",
    "    if CFG.optimizer == \"Adam\":\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=CFG.lr)\n",
    "    else:\n",
    "        raise ValueError(\"Optmizer not found\")\n",
    "    return opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "import tensorflow_extra as tfe\n",
    "\n",
    "# Will download and load pretrained imagenet weights.\n",
    "def build_model(CFG, model_name=None, num_classes=264, compile_model=True):\n",
    "    \"\"\"\n",
    "    Builds and returns a model based on the specified configuration.\n",
    "    \"\"\"\n",
    "    # Create an input layer for the model\n",
    "    inp = tf.keras.layers.Input(shape=(None,))\n",
    "    # Spectrogram\n",
    "    out = tfe.layers.MelSpectrogram(n_mels=CFG.img_size[0],\n",
    "                                    n_fft=CFG.nfft,\n",
    "                                    hop_length=CFG.hop_length, \n",
    "                                    sr=CFG.sample_rate,\n",
    "                                    ref=1.0,\n",
    "                                    out_channels=3)(inp)\n",
    "    # Normalize\n",
    "    out = tfe.layers.ZScoreMinMax()(out)\n",
    "    # TimeFreqMask\n",
    "    out = tfe.layers.TimeFreqMask(freq_mask_prob=0.5,\n",
    "                                  num_freq_masks=1,\n",
    "                                  freq_mask_param=10,\n",
    "                                  time_mask_prob=0.5,\n",
    "                                  num_time_masks=2,\n",
    "                                  time_mask_param=25,\n",
    "                                  time_last=False,)(out)\n",
    "    # Load backbone model\n",
    "    base = getattr(efn, model_name)(input_shape=(None, None, 3),\n",
    "                                            include_top=0,\n",
    "                                            weights=CFG.pretrain,\n",
    "                                           fsr=CFG.fsr)\n",
    "    # Pass the input through the base model\n",
    "    out = base(out)\n",
    "    out = tf.keras.layers.GlobalAveragePooling2D()(out)\n",
    "    out = tf.keras.layers.Dense(num_classes, activation='softmax')(out)\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    if compile_model:\n",
    "        # Set the optimizer\n",
    "        opt = get_optimizer()\n",
    "        # Set the loss function\n",
    "        loss = get_loss()\n",
    "        # Set the evaluation metrics\n",
    "        metrics = get_metrics()\n",
    "        # Compile the model with the specified optimizer, loss function, and metrics\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(CFG, model_name=CFG.model_name)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=CFG.epochs, plot=False):\n",
    "    \"\"\"\n",
    "    Returns a learning rate scheduler callback for a given batch size, mode, and number of epochs.\n",
    "    \"\"\"\n",
    "    # Define the learning rate schedule.\n",
    "    lr_start   = 0.000005\n",
    "    lr_max     = 0.00000140 * batch_size\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    # Function to update the lr\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "\n",
    "        elif CFG.scheduler == 'exp':\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - \\\n",
    "                  lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "        elif CFG.scheduler == 'step':\n",
    "            lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "\n",
    "        elif CFG.scheduler == 'cos':\n",
    "            decay_total_epochs = epochs - lr_ramp_ep - lr_sus_ep + 3\n",
    "            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "            lr = (lr_max - lr_min) * cosine_decay + lr_min\n",
    "        return lr\n",
    "    \n",
    "    # Plot the lr curve\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n",
    "        plt.title('Learning Rate Scheduler')\n",
    "        plt.show()\n",
    "        \n",
    "    # Crate lr-callback to update lr during training\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def wandb_init(fold):\n",
    "    \"\"\"\n",
    "    Initializes the W&B run by creating a config file and initializing a W&B run.\n",
    "    \"\"\"\n",
    "    # Create a dictionary of configuration parameters\n",
    "    config = {k:v for k,v in dict(vars(CFG)).items() if '__' not in k}\n",
    "    config.update({\"fold\":int(fold)}) # int is to convert numpy.int -> int\n",
    "    # Dump the configuration dictionary to a YAML file\n",
    "    yaml.dump(config, open(f'/kaggle/working/config fold-{fold}.yaml', 'w'),)\n",
    "    # Load the configuration dictionary from the YAML file\n",
    "    config = yaml.load(open(f'/kaggle/working/config fold-{fold}.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "    # Initialize a W&B run with the given configuration parameters\n",
    "    run = wandb.init(project=\"birdclef-2023-public\",\n",
    "                     name=f\"fold-{fold}|dim-{CFG.img_size[1]}x{CFG.img_size[0]}|model-{CFG.model_name}\",\n",
    "                     config=config,\n",
    "                     group=CFG.comment,\n",
    "                     save_code=True,)\n",
    "    return run\n",
    "\n",
    "    \n",
    "def log_wandb(valid_df):\n",
    "    \"\"\"Log and save validation results with missclassified examples as audio in W&B\"\"\"\n",
    "    # Query only the rows with miss predictions\n",
    "    save_df = valid_df.query(\"miss==True\")\n",
    "    # Map the predicted and target labels to their corresponding names\n",
    "    save_df.loc[:, 'pred_name'] = save_df.pred.map(CFG.label2name)\n",
    "    save_df.loc[:, 'target_name'] = save_df.target.map(CFG.label2name)\n",
    "    # Trim the dataframe for debugging purposes\n",
    "    if CFG.debug:\n",
    "        save_df = save_df.iloc[:CFG.replicas*CFG.batch_size*CFG.infer_bs]\n",
    "    # Get the columns to be included in the wandb table\n",
    "    noimg_cols = [*CFG.tab_cols, 'target', 'pred', 'target_name','pred_name']\n",
    "    # Retain only the necessary columns\n",
    "    save_df = save_df.loc[:, noimg_cols]\n",
    "\n",
    "    data = []\n",
    "    # Load audio files for each miss prediction\n",
    "    for idx, row in tqdm(save_df.iterrows(), total=len(save_df), desc='wandb ', position=0, leave=True):\n",
    "        filepath = '/kaggle/input/birdclef-2023/train_audio/'+CFG.label2name[row.target]+'/'+row.filename\n",
    "        audio, sr = librosa.load(filepath, sr=None)\n",
    "        # Add the audio file to the data list along with the other relevant information\n",
    "        data+=[[*row.tolist(), wandb.Audio(audio, caption=row.filename, sample_rate=sr)]]\n",
    "    # Create a wandb table with the audio files and other relevant information\n",
    "    wandb_table = wandb.Table(data=data, columns=[*noimg_cols, 'audio'])\n",
    "    # Manually unpack dict values\n",
    "    scores_wb = {f'best.{k}': v for k,v in scores.items()}\n",
    "    # Log the scores and wandb table to wandb\n",
    "    wandb.log({**scores_wb,\n",
    "               'table': wandb_table,\n",
    "               })\n",
    "    \n",
    "# get wandb callbacks\n",
    "def get_wb_callbacks(fold):\n",
    "    wb_ckpt = wandb.keras.WandbModelCheckpoint(filepath='fold-%i.h5'%fold, \n",
    "                                               monitor='val_auc',\n",
    "                                               verbose=CFG.verbose,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=False,\n",
    "                                               mode='max',)\n",
    "    wb_metr = wandb.keras.WandbMetricsLogger()\n",
    "    return [wb_ckpt, wb_metr]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "num_classes = CFG.num_classes2\n",
    "df = df_pre.copy()\n",
    "fold = 0\n",
    "\n",
    "# Compute batch size and number of samples to drop\n",
    "infer_bs = (CFG.batch_size*CFG.infer_bs)\n",
    "drop_remainder = CFG.drop_remainder\n",
    "\n",
    "# Split dataset with cv filter\n",
    "if CFG.cv_filter:\n",
    "    df = filter_data(df, thr=5)\n",
    "    train_df = df.query(\"fold!=@fold | ~cv\").reset_index(drop=True)\n",
    "    valid_df = df.query(\"fold==@fold & cv\").reset_index(drop=True)\n",
    "else:\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "\n",
    "# Upsample train data\n",
    "train_df = upsample_data(train_df, thr=50)\n",
    "train_df = downsample_data(train_df, thr=500)\n",
    "\n",
    "# Get file paths and labels\n",
    "train_paths = train_df.filepath.values; train_labels = train_df.target.values\n",
    "valid_paths = valid_df.filepath.values; valid_labels = valid_df.target.values\n",
    "\n",
    "# Shuffle the file paths and labels\n",
    "index = np.arange(len(train_paths))\n",
    "np.random.shuffle(index)\n",
    "train_paths  = train_paths[index]\n",
    "train_labels = train_labels[index]\n",
    "\n",
    "# For debugging\n",
    "if CFG.debug:\n",
    "    min_samples = CFG.batch_size*CFG.replicas*2\n",
    "    train_paths = train_paths[:min_samples]; train_labels = train_labels[:min_samples]\n",
    "    valid_paths = valid_paths[:min_samples]; valid_labels = valid_labels[:min_samples]\n",
    "    \n",
    "# Ogg or Mp3\n",
    "train_ftype = list(map(lambda x: '.ogg' in x, train_paths))\n",
    "valid_ftype = list(map(lambda x: '.ogg' in x, valid_paths))\n",
    "\n",
    "# Compute the number of training and validation samples\n",
    "num_train = len(train_paths); num_valid = len(valid_paths)\n",
    "\n",
    "# Build the training and validation datasets\n",
    "cache=False\n",
    "train_ds = build_dataset(train_paths, train_ftype, train_labels, \n",
    "                         batch_size=CFG.batch_size*CFG.replicas, cache=cache, shuffle=True,\n",
    "                        drop_remainder=drop_remainder, num_classes=num_classes)\n",
    "valid_ds = build_dataset(valid_paths, valid_ftype, valid_labels,\n",
    "                         batch_size=CFG.batch_size*CFG.replicas, cache=True, shuffle=False,\n",
    "                         augment=False, repeat=False, drop_remainder=drop_remainder,\n",
    "                         take_first=True, num_classes=num_classes)\n",
    "\n",
    "# Print information about the fold and training\n",
    "print('#'*25); print('#### Pre-Training')\n",
    "print('#### Image Size: (%i, %i) | Model: %s | Batch Size: %i | Scheduler: %s'%\n",
    "      (*CFG.img_size, CFG.model_name, CFG.batch_size*CFG.replicas, CFG.scheduler))\n",
    "print('#### Num Train: {:,} | Num Valid: {:,}'.format(len(train_paths), len(valid_paths)))\n",
    "\n",
    "# Clear the session and build the model\n",
    "K.clear_session()\n",
    "with strategy.scope():\n",
    "    model = build_model(CFG, model_name=CFG.model_name, num_classes=num_classes)\n",
    "\n",
    "print('#'*25) \n",
    "\n",
    "# Checkpoint Callback\n",
    "ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'birdclef_pretrained_ckpt.h5', monitor='val_auc', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='max', save_freq='epoch')\n",
    "# LR Scheduler Callback\n",
    "lr_cb = get_lr_callback(CFG.batch_size*CFG.replicas)\n",
    "callbacks = [ckpt_cb, lr_cb]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=2 if CFG.debug else CFG.epochs, \n",
    "    callbacks=callbacks, \n",
    "    steps_per_epoch=len(train_paths)/CFG.batch_size//CFG.replicas,\n",
    "    validation_data=valid_ds, \n",
    "    verbose=CFG.verbose,\n",
    ")\n",
    "\n",
    "# Show training plot\n",
    "if CFG.training_plot:\n",
    "    plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
